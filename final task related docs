1. ros navigation
2. avoid obstacles (move_base, teb_local_planner)
3. curb following
question 1: Robot moving sideways in RVIZ ROS
6

I have found the problem after some long search on the internet. The problem was that in Rviz, X-axis is defined as front of the robot and Y-axis is defined as the sideways. However, in my URDF I defined the robot in a way where Y-axis was its front and X-axis was its sideways. After fixing this, my robot moves normally in Rviz and Gazebo both.

For Perceptin:
IMU+GPS s sensor fusion on localization
plus wheel odeomery   : pose prediction
plus cameras  : localization

Perception:
Object detection
how to detect rigid object
steps:
1. histogram of Orientation(HOG)
2. Support Vector Machine
how to detect non-rigid object
Deforable Part Model(DPM)

Segentation:

Conditional Random Field(CRF)

Stereo,OPtical Flow, and Scene Flow:
stereo vision: Derive depth information using triangulatin techniques, Semi Global Matching(SGM)
Optical Flow: Tranks 2D motion between two images
Scene FLow: Tranks 3D motion between two pairs of sereo images

Object TRanking

MDP: Markovian Decision Process

Traffic Prediction:
1. Classification problem for categorical road object behaviors
2. Regress problem for generating the precited path with speed and time info

Lane -level Routing
1. Modeled as  weighted directed graphs
2. Shortest path problem: Dijkstra and A*
HD MAP has lane info


Behaviroal Decisions:
1. Ruled-based 'divided and Conquer' approach: layered scenarios
2. Markov Decision Process
3. SYnthetic decision and indicidual decisions

Motion Planning:
1. Vehicle model, road model, and SL cooridnation system
2. Path Planning and speed Planning
      Path planning dynamic programming to minimize cost
      Speed planning ST-graph
 
 Feedback control
 1. Bicycle modrl: model vehcicles as rigid bodyies with ront and rear wheels
 2. PID control: proportial -integral-derivative controller
 
 ROS
 Sensors===>computing Platform==>CAN BUS===>COntrol Platform
 
 Heterogeneoius COmputing: autonomous driving
 
 Enabling affordable computer -vision based autonomous vehicles
 
 Fast R-CNN, SSD
 
 PSPNet: Ptramid Scent Parsing
 
 
Flownet


360 images

stereo visual localizaitno: matching previous and current stereo images
GPU, DSP
visual map as reference

: existing digital map+ground feature+spatial features+senmatic information

Enabling COmputer visioned based autonomous vehicles

stereo matchiing: content   CNN

DSP( Sensor Data Processing)==>Thread1(localization), Thread2(localization), GPU(OBJECT RECOGNITION)==>
Thread3(Planning), Thread4(Obstacle avoidance)


Universial HIght PRecision Visual Map
Layer 4: semantic information
Layer 3: spatial features
Layer 2: ground features
Layer 1: Digital map with lane-level annotation


cluod architecture to suppport video streaming and online object recognition tasks and demonmnstrate how alluxio delivers high throughout, low latency, and a unified namespace to support these emerging cloud architectures

enourmous amount of unstructred/  multimeda data generated
heterogeneous underlying storages
require very quickly access to the 'hot' dataset

SLAM+RRT

NVIDIA  end TO end learning for self driving cars
comma.ai: learning a driving simulator
Radford: unsupervised represnetation learning deep gans
NYU: deep multiscale video prediction bveyond mean square error

left camera view+ disparity view + realscale bird view + mask disparity view

create a map of   UCI campus  around 6km/h

77 GHZ radar 5m,10m,  view angle is important

Perceptin calibratino: ethernet connection 

Perceptin: IMU+GNSS



Realtime road condition , vehicle /pedestrain detection, spacial information of environment, depth information of vehicles and pedestrains

https://neilnie.com/2019/04/27/self-driving-golf-cart-autonomous-navigation-with-the-ros-navigation-stack-part-1-mapping/


http://introlab.github.io/rtabmap/

 
 http://docs.ros.org/melodic/api/nav_msgs/html/msg/OccupancyGrid.html
 
 
 osm_cartography ROS Node
 
 
 The commands are sent from the Jetson to an Arduino through serial communication. The Jetson simply processes the visual input from the camera and run a segmentation analysis. You can find out more about segmentation here.

If the segmentation shows that there is an object in the bottom of the view that is 20% size of the image, then the Jetson will send a stopping command to the Arduino.


 carla python3.6 ,3.5 or 2.7
 
 my target: 1. carla simulation  (done)
            2. ros-carla bridge  (python3 or 2)
            3. rosbag---SLAM---roscarla bridge---carla simulation

carla simulation (2.7) // send commands to control a car
ros-carla bridge (2.7) // get messages over topics
SLAM( whatever python3 or 2) // generate prediction commands, sent to ros master


20191009:

zed camera sdk examples, C++,
tutorial1: https://www.stereolabs.com/docs/getting-started/application-development/#building-on-linux-and-jetson


https://www.stereolabs.com/docs/getting-started/application-development/#building-on-linux-and-jetson


1.  CUDA_CUDA_LIBRARY NOTFOUND 
    -DCMAKE_LIBRARY_PATH=/usr/local/cuda/lib64/stubs

2. nvcc not found,
    export PATH=/usr/local/cuda/bin${PATH:+:${PATH}}$ 
    export LD_LIBRARY_PATH=/usr/local/cuda/lib64${LD_LIBRARY_PATH:+:${LD_LIBRARY_PATH}}
3. test positional tracking

4. test spatial mapping

all 1-4 are c++ coding

1. compiling python api for python3
2. test several snippets in tutorials and examples

20191010
failed to image the whole system
locating IMU and SSD for jetson TX2
partially test ZED
20191011:
1. zed-tensorflow, tensorflow-gpu==1.15
2. nearly complete the test of ZED

20191012:
1. initialize Jetson TX2
2. must complete mirroring the ubuntu whole system
3. plan to flash tx2 with ubuntu18

Tx2 flash:
1. install jetpack,sdkmanager in the host machine
2. get to know all ip addresses including host and the jetson tx2
3. manual recover mode
4, mini usb connectkon between host and jetson
5. follow all steps
6. cuda installation into jetson is very slow.
7 installation of componennts is slow because some python packages installed via pypi

20191014





